{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalisation and Overfitting\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand the concept of overfitting and how it relates to model complexity\n",
    "- Visualise how model complexity affects generalisation performance\n",
    "- Implement and evaluate models with different numbers of parameters\n",
    "- Apply early stopping as a regularisation technique to prevent overfitting\n",
    "\n",
    "## Overview\n",
    "In this notebook, we will explore the critical issue of overfitting and learn how to measure how well our trained models generalise to unseen data. This builds upon the generalisation concepts introduced in the fourth lecture. We'll use both a custom regression example and PyTorch to demonstrate key concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Overfitting and Model Complexity in 1D Regression\n",
    "\n",
    "In this exercise, we will explore a regression problem to understand how model complexity affects overfitting. Given a fixed set of noisy observations, we will use multi-layer network models to learn the relationship between inputs and outputs. Our goal is to visualise how increasing model complexity affects the model's ability to make predictions across the input space.\n",
    "\n",
    "### The Target Function\n",
    "\n",
    "To keep things simple, we will consider a single input-output function defined by a fourth-degree polynomial (quartic):\n",
    "\n",
    "$$ f(x) = 10x^4 - 17x^3 + 8x^2 - x $$\n",
    "\n",
    "The observed values are the function values plus zero-mean Gaussian noise:\n",
    "\n",
    "$$ y = f(x) + 0.01\\epsilon \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$\n",
    "\n",
    "The inputs will be drawn from a uniform distribution on the interval $[0, 1]$.\n",
    "\n",
    "**üî• Run the cell below** to import the necessary modules and seed the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plotting style for better visualisations\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "seed = 17102016 \n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù **YOUR TASK: Implement a Polynomial Function**\n",
    "\n",
    "Write code in the cell below to calculate a polynomial function of one-dimensional inputs. \n",
    "\n",
    "If $\\boldsymbol{c}$ is a length $P$ vector of coefficients corresponding to increasing powers in the polynomial (starting from the constant zero-power term up to the $(P-1)^{\\text{th}}$ power), the function should correspond to:\n",
    "\n",
    "$$f_{\\text{polynomial}}(x, \\boldsymbol{c}) = \\sum_{p=0}^{P-1} c_p x^p$$\n",
    "\n",
    "**Requirements:**\n",
    "- The function should take an array of inputs and a coefficient vector\n",
    "- Return the polynomial evaluated at each input point\n",
    "- Handle vectorised operations efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_function(inputs, coefficients):\n",
    "    \"\"\"Calculates polynomial with given coefficients for an array of inputs.\n",
    "    \n",
    "    Args:\n",
    "        inputs: One-dimensional array of input values of shape (num_inputs,)\n",
    "        coefficients: One-dimensional array of polynomial coefficient terms\n",
    "           with `coefficients[0]` corresponding to the coefficient for the\n",
    "           zero-order term (constant) and `coefficients[-1]` corresponding \n",
    "           to the highest order term.\n",
    "           \n",
    "    Returns:\n",
    "        One-dimensional array of output values of shape (num_inputs,)\n",
    "    \n",
    "    Example:\n",
    "        For coefficients [1, 2, 3] and input x, this computes: 1 + 2*x + 3*x^2\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO Implement this function\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üî• Run the cell below** to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the polynomial function implementation\n",
    "test_coefficients = np.array([-1., 3., 4.])  # Represents: -1 + 3x + 4x^2\n",
    "test_inputs = np.array([0., 0.5, 1., 2.])\n",
    "test_outputs = np.array([-1., 1.5, 6., 21.])  # Expected outputs\n",
    "\n",
    "# Check output shape is correct\n",
    "assert polynomial_function(test_inputs, test_coefficients).shape == (4,), (\n",
    "    'Function gives wrong shape output.'\n",
    ")\n",
    "\n",
    "# Check output values are correct\n",
    "assert np.allclose(polynomial_function(test_inputs, test_coefficients), test_outputs), (\n",
    "    'Function gives incorrect output values.'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Function is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Training Data\n",
    "\n",
    "Now we'll use the random number generator to sample input values and calculate the corresponding target outputs using your polynomial implementation. **üî• Run the cell below** to generate the noisy training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the true function coefficients: f(x) = 10x^4 - 17x^3 + 8x^2 - x\n",
    "coefficients = np.array([0, -1., 8., -17., 10.])\n",
    "\n",
    "# Set up problem dimensions\n",
    "input_dim, output_dim = 1, 1\n",
    "noise_std = 0.01  # Standard deviation of Gaussian noise\n",
    "num_data = 80     # Total number of data points\n",
    "\n",
    "# Generate random inputs uniformly distributed in [0, 1]\n",
    "inputs = rng.uniform(size=(num_data, input_dim))\n",
    "\n",
    "# Generate noise samples from standard normal distribution\n",
    "epsilons = rng.normal(size=num_data)\n",
    "\n",
    "# Calculate noisy target outputs: y = f(x) + noise\n",
    "targets = (polynomial_function(inputs[:, 0], coefficients) + \n",
    "           epsilons * noise_std)[:, None]  # Reshape to column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training and Validation Sets\n",
    "\n",
    "We will split the generated data points into equal-sized training and validation datasets. We'll use these to create data provider objects for our framework. Since the dataset is small, we'll use a batch size equal to the dataset size. \n",
    "\n",
    "**üî• Run the cell below** to split the data and set up the data provider objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data provider class\n",
    "from mlp.data_providers import DataProvider\n",
    "\n",
    "# Split data into training and validation sets (50/50 split)\n",
    "num_train = num_data // 2\n",
    "batch_size = num_train  # Use full batch gradient descent\n",
    "\n",
    "# Create training and validation splits\n",
    "inputs_train, targets_train = inputs[:num_train], targets[:num_train]\n",
    "inputs_valid, targets_valid = inputs[num_train:], targets[num_train:]\n",
    "\n",
    "# Create data provider objects for training and validation\n",
    "train_data = DataProvider(inputs_train, targets_train, batch_size=batch_size, rng=rng)\n",
    "valid_data = DataProvider(inputs_valid, targets_valid, batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Data\n",
    "\n",
    "Let's visualise the data we will be modelling. **üî• Run the cell below** to plot the target outputs against inputs for both training and validation sets. Notice the clear underlying smooth functional relationship evident in the noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of the training and validation data\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot training and validation data points\n",
    "ax.plot(inputs_train[:, 0], targets_train[:, 0], '.', label='Training data', alpha=0.7)\n",
    "ax.plot(inputs_valid[:, 0], targets_valid[:, 0], '.', label='Validation data', alpha=0.7)\n",
    "\n",
    "# Add labels and formatting\n",
    "ax.set_xlabel('Inputs $x$', fontsize=14)\n",
    "ax.set_ylabel('Outputs $y$', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.set_title('Training and Validation Data')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function (RBF) Networks\n",
    "\n",
    "We will fit models with varying numbers of parameters to the training data. Since multi-layer logistic sigmoid models tend to perform poorly on regression tasks like this, we will instead use a [Radial Basis Function (RBF) network](https://en.wikipedia.org/wiki/Radial_basis_function_network).\n",
    "\n",
    "**What is an RBF Network?**\n",
    "This model predicts the output as a weighted sum of basis functions (Gaussian-like \"bumps\") tiled across the input space. Each basis function has a centre and width, and the final prediction combines their weighted contributions.\n",
    "\n",
    "**üî• Run the cell below** to see an example of RBF network predictions. Try running it several times with different values of `num_weights` (e.g., 5, 15, 30) to get a feel for how the number of parameters affects the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing this value and re-running the cell to see different behaviours!\n",
    "num_weights = 15\n",
    "weights_scale = 1.\n",
    "bias_scale = 1.\n",
    "\n",
    "def basis_function(x, centre, scale):\n",
    "    \"\"\"Gaussian radial basis function.\"\"\"\n",
    "    return np.exp(-(x - centre)**2 / scale**2)\n",
    "\n",
    "# Generate random weights and bias for demonstration\n",
    "weights = rng.normal(size=num_weights) * weights_scale\n",
    "bias = rng.normal() * bias_scale\n",
    "\n",
    "# Place basis function centres evenly across input space [0, 1]\n",
    "centres = np.linspace(0, 1, weights.shape[0])\n",
    "scale = 1. / weights.shape[0]  # Scale inversely with number of centres\n",
    "\n",
    "# Create dense grid of input points for smooth plotting\n",
    "xs = np.linspace(0, 1, 200)\n",
    "ys = np.zeros(xs.shape[0])\n",
    "\n",
    "# Plot the RBF network prediction\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Sum weighted basis functions\n",
    "for weight, centre in zip(weights, centres):\n",
    "    ys += weight * basis_function(xs, centre, scale)\n",
    "ys += bias  # Add bias term\n",
    "\n",
    "ax.plot(xs, ys, linewidth=2)\n",
    "ax.set_xlabel('Input', fontsize=14)\n",
    "ax.set_ylabel('Output', fontsize=14)\n",
    "ax.set_title(f'RBF Network with {num_weights} basis functions')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation Details\n",
    "\n",
    "You do not need to study the details of how to implement this model. All the additional code you need to fit RBF networks is provided in the `RadialBasisFunctionLayer` in the `mlp.layers` module. \n",
    "\n",
    "**Key Points:**\n",
    "- The `RadialBasisFunctionLayer` class has the same interface as other layer classes (with `fprop` and `bprop` methods)\n",
    "- We can include it as a layer in a `MultipleLayerModel` just like any other layer\n",
    "- This demonstrates the advantage of using a modular framework - we can reuse existing code to train different model architectures\n",
    "\n",
    "**Architecture:** We use the `RadialBasisFunctionLayer` as the first layer in a two-layer model:\n",
    "1. **First layer:** `RadialBasisFunctionLayer` - calculates the basis function terms\n",
    "2. **Second layer:** `AffineLayer` - weights and sums the basis functions together\n",
    "\n",
    "**üî• Run the cell below** to set up the necessary components for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for model training\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.layers import AffineLayer, RadialBasisFunctionLayer\n",
    "from mlp.errors import SumOfSquaredDiffsError\n",
    "from mlp.initialisers import ConstantInit, UniformInit\n",
    "from mlp.learning_rules import GradientDescentLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "\n",
    "# Set up training components\n",
    "error = SumOfSquaredDiffsError()  # Appropriate for regression problems\n",
    "learning_rule = GradientDescentLearningRule(0.1)  # Basic gradient descent with fixed learning rate\n",
    "\n",
    "# Initialise weights and biases\n",
    "weights_init = UniformInit(-0.1, 0.1)  # Small random weights\n",
    "biases_init = ConstantInit(0.)         # Zero bias initialization\n",
    "\n",
    "# Training configuration\n",
    "num_epoch = 2000  # Number of training epochs for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models with Different Complexities\n",
    "\n",
    "The next cell defines RBF network models with varying numbers of weight parameters (equal to the number of basis functions) and fits each to the training set. We'll record the final training and validation set errors for the fitted models.\n",
    "\n",
    "**üî• Run the cell below** to fit the models and calculate error values. This may take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different model complexities to test\n",
    "num_weight_list = [2, 5, 10, 25, 50, 100]\n",
    "\n",
    "# Storage for results\n",
    "models = []\n",
    "train_errors = []\n",
    "valid_errors = []\n",
    "\n",
    "# Train models with different numbers of parameters\n",
    "for num_weight in num_weight_list:\n",
    "    # Create RBF network model\n",
    "    model = MultipleLayerModel([\n",
    "        RadialBasisFunctionLayer(num_weight),\n",
    "        AffineLayer(input_dim * num_weight, output_dim, \n",
    "                    weights_init, biases_init)\n",
    "    ])\n",
    "    \n",
    "    # Set up optimiser\n",
    "    optimiser = Optimiser(model, error, learning_rule, \n",
    "                            train_data, valid_data)\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print(f'Training model with {num_weight} weights')\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # Train the model\n",
    "    _ = optimiser.train(num_epoch, -1)  # -1 means no intermediate output\n",
    "    \n",
    "    # Calculate final errors on both datasets\n",
    "    outputs_train = model.fprop(inputs_train)[-1]\n",
    "    outputs_valid = model.fprop(inputs_valid)[-1]\n",
    "    \n",
    "    # Store results\n",
    "    models.append(model)\n",
    "    train_errors.append(error(outputs_train, targets_train))\n",
    "    valid_errors.append(error(outputs_valid, targets_valid))\n",
    "    \n",
    "    print(f'  Final training set error: {train_errors[-1]:.1e}')\n",
    "    print(f'  Final validation set error: {valid_errors[-1]:.1e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù **YOUR TASK: Analyse Training vs Validation Errors**\n",
    "\n",
    "In the cell below, write code to create [bar charts](http://matplotlib.org/examples/api/barchart_demo.html) showing the training and validation set errors for the different fitted models.\n",
    "\n",
    "**Think about these questions as you examine the plots:**\n",
    "\n",
    "1. **Model Complexity:** Do models with more free parameters fit the training data better or worse?\n",
    "2. **Generalisation:** What does the validation set error tell us about how well the models generalise?\n",
    "3. **Best Model:** Which of the fitted models seems most likely to generalise well to unseen data?\n",
    "4. **Overfitting:** Do any of the models appear to be overfitting? How can you tell?\n",
    "\n",
    "**Hint:** Look for the \"sweet spot\" where validation error is minimised!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO plot the bar charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù **YOUR TASK: Visualise Model Predictions**\n",
    "\n",
    "Now let's visualise what the fitted models' predictions look like across the whole input space compared to the true function we were trying to fit.\n",
    "\n",
    "**In the cell below, complete the following for each fitted model:**\n",
    "1. Compute output predictions for the model across 500 linearly spaced input points between 0 and 1\n",
    "2. Plot the predicted outputs and true function values as line plots on the same axis\n",
    "3. Plot the training data as points on the same axis\n",
    "4. Add appropriate labels and legends\n",
    "\n",
    "**Look for:**\n",
    "- **Underfitting:** Model is too simple and misses the true pattern\n",
    "- **Good fit:** Model captures the underlying function well\n",
    "- **Overfitting:** Model fits training points perfectly but behaves erratically between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO plot the graphs here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to relate your answers to the questions above to what you see in these plots - ask a demonstrator if you are unsure what is going on. In particular for the models which appeared to be overfitting and generalising poorly you should now have an idea how this looks in terms of the model's predictions and how these relate to the training data points and true function values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Early Stopping with PyTorch\n",
    "\n",
    "In the previous exercise with RBF networks, we saw how model complexity affects overfitting. We observed that:\n",
    "- Simple models (few parameters) underfit the data\n",
    "- Complex models (many parameters) overfit the training data\n",
    "- There's an optimal complexity that minimises validation error\n",
    "\n",
    "However, there's another way to think about overfitting: **through the lens of training time**.\n",
    "\n",
    "## Overfitting and Training Duration\n",
    "\n",
    "As we saw in [Lab 3](https://github.com/cortu01/mlpractical/tree/mlp2023-24/lab3/notebooks/03_Multiple_layer_models.ipynb), models can show signs of overfitting after training for too many epochs. Even with an appropriately sized model, overfitting can occur when we train for too long.\n",
    "\n",
    "**Key Insight:** Overfitting happens when the model learns the training data *too well*, including its noise, and fails to generalise to unseen data. This can result from:\n",
    "1. **Model complexity** (too many parameters) - as we saw in Exercise 1\n",
    "2. **Training duration** (too many epochs) - which we'll explore now\n",
    "\n",
    "## ü§î **Think About This:**\n",
    "*If we observe both high training error AND high validation error, what does this suggest about our model?*\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "**Answer:** High training error + high validation error typically indicates **underfitting**. The model is too simple to capture the underlying patterns in the data, so it performs poorly on both training and validation sets.\n",
    "\n",
    "</details>\n",
    "\n",
    "## Early Stopping: A Regularisation Technique\n",
    "\n",
    "**Early stopping** is a simple yet effective technique to prevent overfitting. The idea is to monitor the validation error during training and stop when it starts to increase consistently, even if the training error continues to decrease.\n",
    "\n",
    "In this section, we'll implement early stopping in PyTorch using the MNIST dataset and demonstrate how it can prevent overfitting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration - use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set training hyperparameters\n",
    "batch_size = 128      # Number of data points in each batch\n",
    "learning_rate = 0.001 # Learning rate for gradient descent\n",
    "num_epochs = 50       # Maximum number of training epochs\n",
    "stats_interval = 1    # Epoch interval for recording and printing statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations (normalisation for MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # MNIST normalisation: mean=0.1307, std=0.3081 (computed from training set)\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST datasets\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create train/validation split from training set\n",
    "valid_size = 0.2  # Use 20% of training set for validation\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "np.random.shuffle(indices)  # Shuffle indices for random split\n",
    "\n",
    "# Split indices into training and validation sets\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=train_sampler, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=valid_sampler, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_idx)}\")\n",
    "print(f\"Validation samples: {len(valid_idx)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class MultipleLayerModel(nn.Module):\n",
    "    \"\"\"Multiple layer model for MNIST classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()  # Flatten 28x28 images to 784-dimensional vectors\n",
    "        \n",
    "        # Define the network architecture\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),   # First hidden layer\n",
    "            nn.ReLU(),                          # ReLU activation\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # Second hidden layer\n",
    "            nn.ReLU(),                          # ReLU activation\n",
    "            nn.Linear(hidden_dim, output_dim),  # Output layer\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = self.flatten(x)              # Flatten input images\n",
    "        logits = self.linear_relu_stack(x)  # Pass through network layers\n",
    "        return logits\n",
    "\n",
    "# Model configuration\n",
    "input_dim = 1 * 28 * 28  # MNIST images are 28x28 pixels\n",
    "output_dim = 10          # 10 classes (digits 0-9)\n",
    "hidden_dim = 100         # Hidden layer size\n",
    "\n",
    "# Create model and move to device\n",
    "model = MultipleLayerModel(input_dim, output_dim, hidden_dim).to(device)\n",
    "\n",
    "# Define loss function and optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimiser\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Early Stopping\n",
    "\n",
    "Early stopping monitors the validation loss during training and stops when it hasn't improved for a specified number of epochs (called \"patience\").\n",
    "\n",
    "**How it works:**\n",
    "1. **Monitor validation loss** after each epoch\n",
    "2. **Track the best (lowest) validation loss** seen so far\n",
    "3. **Count consecutive epochs** without improvement\n",
    "4. **Stop training** when patience is exceeded\n",
    "\n",
    "## ü§î **Think About This:**\n",
    "*Can we say that overfitting is ultimately inevitable given training over a very large number of epochs?*\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "**Answer:** Generally yes, for most practical scenarios. Given unlimited training time, a sufficiently complex model will eventually memorise the training data perfectly, including noise. This leads to overfitting. Early stopping prevents this by halting training before this point is reached.\n",
    "\n",
    "</details>\n",
    "\n",
    "**üî• Run the cell below** to see the implementation of an early stopping class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility to prevent overfitting during training.\n",
    "    \n",
    "    Monitors validation loss and stops training when it hasn't improved\n",
    "    for a specified number of epochs (patience).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs with no improvement after which \n",
    "                          training will be stopped\n",
    "            min_delta (float): Minimum change in monitored quantity to qualify \n",
    "                             as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0  # Counter for epochs without improvement\n",
    "        self.min_validation_loss = float('inf')  # Best validation loss seen\n",
    "        self.early_stop = False  # Flag to indicate if training should stop\n",
    "\n",
    "    def __call__(self, validation_loss):\n",
    "        \"\"\"Check if training should be stopped based on validation loss.\n",
    "        \n",
    "        Args:\n",
    "            validation_loss (float): Current epoch's validation loss\n",
    "        \"\"\"\n",
    "        # Check if we have a new best validation loss\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0  # Reset counter\n",
    "        # Check if validation loss has worsened beyond tolerance\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            # Stop training if patience exceeded\n",
    "            if self.counter >= self.patience: \n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize early stopping with patience=5 epochs\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.01)\n",
    "\n",
    "# Track loss values over training\n",
    "train_losses = [] \n",
    "valid_losses = []\n",
    "\n",
    "print(\"Starting training with early stopping...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs + 1): \n",
    "    # === TRAINING PHASE ===\n",
    "    model.train()  # Set model to training mode\n",
    "    batch_losses = []\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimisation\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update parameters\n",
    "        \n",
    "        # Record batch loss\n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # Average training loss for this epoch\n",
    "    train_losses.append(np.mean(batch_losses))\n",
    "\n",
    "    # === VALIDATION PHASE ===\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    batch_losses = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for batch_idx, (data, targets) in enumerate(valid_loader):\n",
    "            # Move data to device\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass only\n",
    "            outputs = model(data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            # Record batch loss\n",
    "            batch_losses.append(loss.item())\n",
    "    \n",
    "    # Average validation loss for this epoch\n",
    "    valid_losses.append(np.mean(batch_losses))\n",
    "\n",
    "    # Print progress every stats_interval epochs\n",
    "    if epoch % stats_interval == 0:\n",
    "        print(f'Epoch {epoch:2d}: Train Loss: {train_losses[-1]:.6f}, '\n",
    "              f'Valid Loss: {valid_losses[-1]:.6f}')\n",
    "            \n",
    "    # Check for early stopping\n",
    "    early_stopping(valid_losses[-1])\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\nüõë Early stopping triggered at epoch {epoch}\")\n",
    "        print(f\"Best validation loss: {early_stopping.min_validation_loss:.6f}\")\n",
    "        break\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss curves\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot loss curves\n",
    "epochs = range(len(train_losses))\n",
    "ax.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "ax.plot(epochs, valid_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "\n",
    "# Mark the early stopping point\n",
    "if early_stopping.early_stop:\n",
    "    ax.axvline(x=len(train_losses)-1, color='orange', linestyle='--', \n",
    "               linewidth=2, label='Early Stopping Point')\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Loss', fontsize=14)\n",
    "ax.set_title('Training and Validation Loss with Early Stopping', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"Training stopped at epoch: {len(train_losses)-1}\")\n",
    "print(f\"Final training loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final validation loss: {valid_losses[-1]:.6f}\")\n",
    "print(f\"Best validation loss: {early_stopping.min_validation_loss:.6f}\")\n",
    "\n",
    "# Show the classic overfitting pattern\n",
    "min_valid_idx = np.argmin(valid_losses)\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "print(f\"Validation loss minimum at epoch: {min_valid_idx}\")\n",
    "if len(valid_losses) > min_valid_idx + 3:\n",
    "    print(\"‚úÖ Early stopping successfully prevented overfitting!\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Training stopped before significant overfitting occurred.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
