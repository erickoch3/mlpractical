{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01a: Practice — Build a Feed‑Forward Network (FFN) from Scratch\n\n",
        "This notebook is a hands‑on scaffold to implement a fully‑connected feed‑forward neural network step by step using NumPy. Each section provides:\n\n",
        "- Empty functions to fill in (raise `NotImplementedError`)\n",
        "- Clear type hints and expected shapes/dtypes\n",
        "- Concise algorithm guidance + math in LaTeX\n",
        "- Historical notes about when/why ideas emerged\n\n",
        "By the end, you’ll be able to train a small FFN on MNIST using the utilities in this repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Historical Context\n\n",
        "- Perceptron (single linear threshold unit): Frank Rosenblatt, 1957. Momentum grew with hardware of the time but limited by linear separability.\n",
        "- Perceptron limits: Marvin Minsky & Seymour Papert, 1969, highlighted non‑linearly separable problems (e.g., XOR).\n",
        "- Backpropagation (general idea/chain rule): Paul Werbos (1974, thesis); popularized by Rumelhart, Hinton, Williams (1986). Compute improvements and matrix libraries made multilayer training practical.\n",
        "- Softmax + cross‑entropy (multiclass logistic regression) widely adopted by late 1980s/1990s; normalized exponential/\"softmax\" connections in statistical modeling matured in this era.\n",
        "- Universal Approximation: Cybenko (1989); Hornik (1989–1991).\n",
        "- ReLU nonlinearity: used earlier in neuroscience; popularized for deep nets by Nair & Hinton (2010) and Glorot et al. (2011), enabling better gradient flow.\n",
        "- Initialization: Glorot/Xavier (2010) and He (2015) stabilizing signal propagation at scale.\n",
        "- Optimization: SGD formalized by Robbins & Monro (1951); momentum by Polyak (1964); Adam by Kingma & Ba (2014).\n",
        "- Modern deep learning acceleration: commodity GPUs (mid‑late 2000s), AlexNet (2012) demonstrated compute + data + architecture synergy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notation and Math\n\n",
        "We consider an L‑layer MLP (L hidden layers; output layer counts as layer L+1):\n\n",
        "- Input: $X \\in \\mathbb{R}^{N \\times d_0}$ (mini‑batch of N)\n",
        "- Weights: $W^{(l)} \\in \\mathbb{R}^{d_{l-1} \\times d_l}$, biases: $b^{(l)} \\in \\mathbb{R}^{d_l}$\n",
        "- Pre‑activation: $Z^{(l)} = X^{(l-1)} W^{(l)} + b^{(l)}$\n",
        "- Activation: $A^{(l)} = f(Z^{(l)})$ (ReLU in hidden layers)\n",
        "- Output logits: $O = A^{(L)} W^{(L+1)} + b^{(L+1)}$\n",
        "- Softmax: $P_i = \\mathrm{softmax}(O_i) = \\frac{e^{O_i - \\mathrm{max}(O_i)}}{\\sum_j e^{O_{ij} - \\mathrm{max}(O_i)}}$\n",
        "- Cross‑entropy: $\\mathcal{L} = - \\frac{1}{N} \\sum_i \\log P_{i,y_i}$\n\n",
        "Key gradients for training with backpropagation:\n\n",
        "- For softmax + cross‑entropy with integer labels $y$: $\\frac{\\partial \\mathcal{L}}{\\partial O} = \\frac{1}{N}(P - \\mathrm{one\\text{-}hot}(y))$\n",
        "- Linear layer: given $Y = XW + b$ and upstream $G = \\partial \\mathcal{L} / \\partial Y$, then\\\n$\\quad \\partial \\mathcal{L}/\\partial W = X^\\top G, \\quad \\partial \\mathcal{L}/\\partial b = \\sum_{i=1}^N G_i, \\quad \\partial \\mathcal{L}/\\partial X = G W^\\top$\n",
        "- ReLU: $\\mathrm{ReLU}(z)=\\max(0,z)$, grad mask $M=\\mathbf{1}[z>0]$, so $\\partial \\mathcal{L}/\\partial Z = (\\partial \\mathcal{L}/\\partial A) \\odot M$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "from __future__ import annotations\n",
        "import math\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "np.random.seed(42)\n",
        "Float = np.float32\n",
        "Int = np.int64\n",
        "\n",
        "# Optional: dataset utilities from this repo (MNIST)\n",
        "from mlp import data_providers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Initialization\n\n",
        "Goal: create parameter dictionaries with correct shapes/dtypes and variance‑scaled random init.\n\n",
        "- Use `float32` everywhere for speed and to mirror practical training.\n",
        "- For each layer l: `W_l: (d_{l-1}, d_l)`, `b_l: (d_l,)`.\n",
        "- Xavier/Glorot (2010) normal: $\\sigma = \\sqrt{2/(fan\\_in + fan\\_out)}$\n",
        "- He (2015) normal: $\\sigma = \\sqrt{2/fan\\_in}$ (good for ReLU).\n\n",
        "Historical note: before variance‑scaled inits, deeper nets suffered from vanishing/exploding signals; Glorot (2010) and He (2015) made stable deep training much more accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_parameters(layer_sizes: List[int], method: str = 'he', seed: int | None = 42) -> Dict[str, NDArray[Float]]:\n",
        "    \"\"\"\n",
        "    Create parameters for an MLP.\n",
        "\n",
        "    Args:\n",
        "        layer_sizes: [d_0, d_1, ..., d_L, d_{L+1}] including input and output dims.\n",
        "        method: 'xavier' or 'he'.\n",
        "        seed: optional RNG seed for reproducibility.\n",
        "\n",
        "    Returns: dict with keys 'W1','b1',...,'W{L+1}','b{L+1}', each NDArray[float32].\n",
        "\n",
        "    Algorithm (fill in):\n",
        "        - For each l in 1..L+1, set fan_in=layer_sizes[l-1], fan_out=layer_sizes[l].\n",
        "        - If method == 'xavier': std = sqrt(2.0 / (fan_in + fan_out)).\n",
        "          If method == 'he':     std = sqrt(2.0 / fan_in).\n",
        "        - W_l ~ Normal(0, std^2), dtype float32.\n",
        "        - b_l = zeros(fan_out, dtype float32).\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Linear Layer (Affine Transform)\n\n",
        "$Z = XW + b$\n\n",
        "- Shapes: `X: (N, d_in)`, `W: (d_in, d_out)`, `b: (d_out,)` -> `Z: (N, d_out)`\n",
        "- Dtypes: `float32`\n",
        "- Implement vectorized matrix multiply and bias add.\n\n",
        "Historically, fast BLAS libraries and GPUs massively accelerated this step, enabling practical deep nets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linear_forward(X: NDArray[Float], W: NDArray[Float], b: NDArray[Float]) -> NDArray[Float]:\n",
        "    \"\"\"Compute Z = X @ W + b, returning float32 with shape (N, d_out).\n",
        "    Args: X(N,d_in), W(d_in,d_out), b(d_out,) all float32.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Nonlinearity (ReLU)\n\n",
        "$\\mathrm{ReLU}(z) = \\max(0, z)$\n\n",
        "- Forward: elementwise max.\n",
        "- Backward: mask where $z>0$. Popularized ~2010–2011, ReLU improved gradient flow vs. sigmoids/tanh in deep nets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu_forward(Z: NDArray[Float]) -> NDArray[Float]:\n",
        "    \"\"\"Return A = max(Z, 0) with same shape/dtype.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def relu_backward(dA: NDArray[Float], Z: NDArray[Float]) -> NDArray[Float]:\n",
        "    \"\"\"Given upstream dA and preactivation Z, return dZ = dA * (Z>0).\n",
        "    Shapes: both (N, d), dtype float32.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Softmax and Cross‑Entropy\n\n",
        "Use numerically stable softmax by subtracting per‑row max.\n\n",
        "$P_i = \\mathrm{softmax}(O_i) = \\frac{e^{O_i - m_i}}{\\sum_j e^{O_{ij} - m_i}}, \\quad m_i = \\max_j O_{ij}$\n\n",
        "Cross‑entropy with integer labels $y$: $\\mathcal{L} = -\\tfrac{1}{N} \\sum_i \\log P_{i,y_i}$.\n\n",
        "Gradient: $\\partial \\mathcal{L} / \\partial O = (P - one\\text{-}hot(y)) / N$.\n\n",
        "Softmax regression for multiclass classification became standard by the late 1980s/1990s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(logits: NDArray[Float]) -> NDArray[Float]:\n",
        "    \"\"\"Return row‑wise softmax probabilities with float32 shape (N, C).\n",
        "    Use stability trick: subtract row max before exp.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def cross_entropy_loss(probs: NDArray[Float], y: NDArray[Int]) -> Tuple[Float, NDArray[Float]]:\n",
        "    \"\"\"Compute mean cross‑entropy and per‑example negative log‑likelihoods.\n",
        "    Args: probs (N,C) float32 with rows summing to 1; y (N,) int64 class indices.\n",
        "    Returns: (mean_loss: float32 scalar, nll: (N,) float32).\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def softmax_ce_backward(logits: NDArray[Float], y: NDArray[Int]) -> NDArray[Float]:\n",
        "    \"\"\"Return dL/dlogits for softmax + mean cross‑entropy.\n",
        "    Implement: probs = softmax(logits); grad = (probs - one_hot(y, C)) / N.\n",
        "    Shape: (N, C), dtype float32.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def one_hot(y: NDArray[Int], num_classes: int) -> NDArray[Float]:\n",
        "    \"\"\"Return float32 one‑hot matrix of shape (N, num_classes).\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Linear Backward\n\n",
        "For $Y = XW + b$ with upstream $G = \\partial \\mathcal{L}/\\partial Y$:\n\n",
        "$\\quad \\partial \\mathcal{L}/\\partial W = X^\\top G, \\ \\partial \\mathcal{L}/\\partial b = \\sum_{i=1}^N G_i, \\ \\partial \\mathcal{L}/\\partial X = G W^\\top$\n\n",
        "Implement vectorized gradients with float32 types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linear_backward(dY: NDArray[Float], X: NDArray[Float], W: NDArray[Float]) -> Tuple[NDArray[Float], NDArray[Float], NDArray[Float]]:\n",
        "    \"\"\"Given upstream dY and cached X, W, compute (dX, dW, db).\n",
        "    Shapes: dY(N,d_out), X(N,d_in), W(d_in,d_out).\n",
        "    Return: dX(N,d_in), dW(d_in,d_out), db(d_out,). All float32.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Model Forward/Backward (2‑Hidden Example)\n\n",
        "Wire linear + ReLU blocks for a small MLP. Example architecture: `d0 -> d1 -> d2 -> C`.\n",
        "- Forward should return logits and caches needed for backward.\n",
        "- Backward should produce grads dict matching params keys.\n\n",
        "Backpropagation was computationally onerous in early decades; maturing compute and frameworks in the 2000s–2010s made this routine development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ffn_forward_2layer(params: Dict[str, NDArray[Float]], X: NDArray[Float]) -> Tuple[NDArray[Float], dict]:\n",
        "    \"\"\"Forward pass for 2‑hidden‑layer MLP using ReLU in hidden layers.\n",
        "    Expected params keys: W1,b1,W2,b2,W3,b3 for dims d0->d1->d2->C.\n",
        "    Returns: (logits: (N,C), cache: dict of intermediates for backward).\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def ffn_backward_2layer(cache: dict, y: NDArray[Int], params: Dict[str, NDArray[Float]]) -> Dict[str, NDArray[Float]]:\n",
        "    \"\"\"Backward pass producing grads dict with keys dW1,db1,dW2,db2,dW3,db3.\n",
        "    Use softmax_ce_backward for final layer gradient.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Metrics and Updates\n\n",
        "- Accuracy: `argmax` of probabilities vs labels.\n",
        "- SGD update: $\\theta \\leftarrow \\theta - \\eta \\cdot g$ (momentum optional — Polyak, 1964)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(probs: NDArray[Float], y: NDArray[Int]) -> Float:\n",
        "    \"\"\"Compute float32 accuracy in [0,1].\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "def sgd_update(params: Dict[str, NDArray[Float]], grads: Dict[str, NDArray[Float]], lr: float = 1e-2) -> None:\n",
        "    \"\"\"In‑place SGD update: params[key] -= lr * grads['d'+key].\n",
        "    All arrays are float32; cast lr to float32 during multiplication.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Training Loop (MNIST)\n\n",
        "Use the provided `mlp.data_providers.MNISTDataProvider` to fetch mini‑batches. Implement the loop once your functions work.\n\n",
        "Pseudocode:\n\n",
        "1. Init params with sizes `[784, 128, 64, 10]`.\n",
        "2. For each epoch and batch:\n",
        "   - Forward -> logits -> probs\n",
        "   - Loss, accuracy\n",
        "   - Backward -> grads\n",
        "   - Update params (SGD)\n",
        "3. Track train/valid metrics.\n\n",
        "Compute note: large batch sizes grew practical with GPUs; historically, small batches/stochastic updates were a pragmatic default on CPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_mnist_2layer(hidden_sizes: Tuple[int,int] = (128,64), lr: float = 1e-1, epochs: int = 3, batch_size: int = 128,\n",
        "                        max_train_batches: int | None = 200, max_valid_batches: int | None = 50, init: str = 'he') -> Dict[str, float]:\n",
        "    \"\"\"Skeleton training loop for MNIST. Fill in once primitives are implemented.\n",
        "    Returns history dict with last seen metrics (you may extend).\n",
        "\n",
        "    Steps to implement:\n",
        "      1) Build data providers: train/valid via data_providers.MNISTDataProvider.\n",
        "      2) Initialize params for sizes [784, h1, h2, 10].\n",
        "      3) For each epoch and batch: forward, loss, backward, SGD update.\n",
        "      4) Periodically evaluate on valid set.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Quick Sanity Checks (Suggested)\n\n",
        "- Shapes: verify each function returns expected shapes.\n",
        "- Dtypes: ensure float32 throughout to avoid accidental float64 upcasts.\n",
        "- Numerical stability: confirm softmax rows sum to ~1 and no NaNs.\n",
        "- Overfit a tiny subset (e.g., 256 samples) to confirm learning dynamics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References (Pointer‑Style)\n\n",
        "- Rosenblatt, F. (1957). The Perceptron.\n",
        "- Minsky, M., Papert, S. (1969). Perceptrons.\n",
        "- Werbos, P. (1974). Beyond Regression: New Tools for Prediction and Analysis.\n",
        "- Rumelhart, D., Hinton, G., Williams, R. (1986). Learning representations by back‑propagating errors.\n",
        "- Cybenko, G. (1989); Hornik, K. (1989–1991): Universal approximation results.\n",
        "- Glorot, X., Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks.\n",
        "- Nair, V., Hinton, G. (2010); Glorot et al. (2011): ReLU in deep nets.\n",
        "- He, K. et al. (2015). Delving deep into rectifiers.\n",
        "- Kingma, D., Ba, J. (2014). Adam.\n",
        "- Krizhevsky, A., Sutskever, I., Hinton, G. (2012). ImageNet classification with deep CNNs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
