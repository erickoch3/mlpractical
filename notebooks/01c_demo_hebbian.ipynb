{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01c: Hebbian Spiking NN (MNIST)\n\n",
        "This notebook explores a simple Hebbian \"fire together, wire together\" learning approach on MNIST with a two-layer spiking network and competition to avoid all-to-all activation.\n\n",
        "- Encoding: Poisson spike trains from pixel intensities.\n",
        "- Neurons: LIF-like integration with soft reset in hidden and output layers.\n",
        "- Competition: k-WTA (top-k) at hidden and winner-take-all at output.\n",
        "- Learning: Oja's rule for input→hidden; supervised Hebbian + anti-Hebbian for hidden→output with teacher-forced spikes.\n",
        "- Evaluation: Argmax of output spike counts.\n\n",
        "This is a didactic example focused on clarity, not performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n\n",
        "# Local package\n",
        "import mlp.data_providers as data_providers\n\n",
        "# Ensure MLP_DATA_DIR is set by walking up to find the repo's data/ folder\n",
        "def ensure_mlp_data_dir():\n",
        "    if os.environ.get('MLP_DATA_DIR'):\n",
        "        return Path(os.environ['MLP_DATA_DIR'])\n",
        "    here = Path.cwd().resolve()\n",
        "    for p in [here] + list(here.parents):\n",
        "        candidate = p / 'data'\n",
        "        if (candidate / 'mnist-train.npz').exists():\n",
        "            os.environ['MLP_DATA_DIR'] = str(candidate)\n",
        "            return candidate\n",
        "    raise RuntimeError('Could not locate data directory with mnist-*.npz')\n\n",
        "data_dir = ensure_mlp_data_dir()\n",
        "print('Using data dir:', data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training/validation providers\n",
        "train_dp = data_providers.MNISTDataProvider('train', batch_size=64, max_num_batches=100, shuffle_order=True)\n",
        "valid_dp = data_providers.MNISTDataProvider('valid', batch_size=64, max_num_batches=50, shuffle_order=False)\n",
        "print('Batches per epoch (train, valid):', train_dp.len(), valid_dp.len())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utilities: scaling and Poisson encoding\n",
        "def scale01(x: np.ndarray) -> np.ndarray:\n",
        "    # MNIST may already be in [0,1], but guard just in case\n",
        "    xmax = np.max(x)\n",
        "    if xmax > 1.5:\n",
        "        return (x / 255.0).astype(np.float32)\n",
        "    return x.astype(np.float32)\n\n",
        "def poisson_encode(inputs: np.ndarray, T: int, rate_hz: float = 30.0, dt: float = 0.005, rng: np.random.RandomState | None = None) -> np.ndarray:\n",
        "    \"\"\"Encode inputs in [0,1] into Poisson spikes over T steps.\n",
        "    inputs: (B, D) in [0,1]\n",
        "    returns: spikes (B, T, D) with 0/1 values\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.RandomState(0)\n",
        "    B, D = inputs.shape\n",
        "    lam = np.clip(inputs, 0.0, 1.0) * rate_hz * dt  # probability per bin\n",
        "    spikes = rng.rand(B, T, D) < lam[:, None, :]\n",
        "    return spikes.astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Two-layer Hebbian SNN with competition and decay\n",
        "def k_wta(v: np.ndarray, k: int) -> np.ndarray:\n",
        "    if k is None or k <= 0:\n",
        "        return np.zeros_like(v, dtype=np.float32)\n",
        "    k = min(k, v.shape[0])\n",
        "    idx = np.argpartition(v, -k)[-k:]\n",
        "    spikes = np.zeros_like(v, dtype=np.float32)\n",
        "    spikes[idx] = (v[idx] > 0).astype(np.float32)\n",
        "    return spikes\n\n",
        "class TwoLayerHebbianSNN:\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int,\n",
        "                 dt: float = 0.005, tau_h: float = 0.02, tau_o: float = 0.02,\n",
        "                 vth_h: float = 1.0, vth_o: float = 1.0, seed: int = 0,\n",
        "                 k_hidden: int = 32, k_output: int = 1):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.dt = dt\n",
        "        self.alpha_h = float(np.exp(-dt / tau_h))\n",
        "        self.alpha_o = float(np.exp(-dt / tau_o))\n",
        "        self.vth_h = vth_h\n",
        "        self.vth_o = vth_o\n",
        "        self.k_hidden = k_hidden\n",
        "        self.k_output = k_output\n",
        "        rng = np.random.RandomState(seed)\n",
        "        self.W1 = (0.01 * rng.randn(input_dim, hidden_dim)).astype(np.float32)\n",
        "        self.W2 = (0.01 * rng.randn(hidden_dim, num_classes)).astype(np.float32)\n\n",
        "    def forward_counts(self, spikes: np.ndarray) -> np.ndarray:\n",
        "        B, T, D = spikes.shape\n",
        "        assert D == self.input_dim\n",
        "        counts = np.zeros((B, self.num_classes), dtype=np.float32)\n",
        "        for b in range(B):\n",
        "            v_h = np.zeros(self.hidden_dim, dtype=np.float32)\n",
        "            v_o = np.zeros(self.num_classes, dtype=np.float32)\n",
        "            for t in range(T):\n",
        "                x_t = spikes[b, t]\n",
        "                v_h = self.alpha_h * v_h + x_t @ self.W1\n",
        "                s_h = k_wta(v_h, self.k_hidden)\n",
        "                v_h = v_h - self.vth_h * s_h\n",
        "                v_o = self.alpha_o * v_o + s_h @ self.W2\n",
        "                s_o = k_wta(v_o, self.k_output)\n",
        "                v_o = v_o - self.vth_o * s_o\n",
        "                counts[b] += s_o\n",
        "        return counts\n\n",
        "    def predict(self, spikes: np.ndarray) -> np.ndarray:\n",
        "        counts = self.forward_counts(spikes)\n",
        "        return counts.argmax(axis=1)\n\n",
        "    def train_step(self, spikes: np.ndarray, y_int: np.ndarray,\n",
        "                   eta1: float = 0.01, eta2: float = 0.02, eta2_neg: float = 0.01,\n",
        "                   teacher_rate_hz: float = 60.0, decay1: float = 5e-5, decay2: float = 5e-5,\n",
        "                   rng: np.random.RandomState | None = None):\n",
        "        if rng is None:\n",
        "            rng = np.random.RandomState(0)\n",
        "        B, T, D = spikes.shape\n",
        "        C = self.num_classes\n",
        "        p_teacher = min(1.0, teacher_rate_hz * self.dt)\n",
        "        for b in range(B):\n",
        "            v_h = np.zeros(self.hidden_dim, dtype=np.float32)\n",
        "            v_o = np.zeros(C, dtype=np.float32)\n",
        "            yb = int(y_int[b])\n",
        "            for t in range(T):\n",
        "                x_t = spikes[b, t]  # (D,)\n",
        "                # Hidden dynamics\n",
        "                v_h = self.alpha_h * v_h + x_t @ self.W1\n",
        "                s_h = k_wta(v_h, self.k_hidden)\n",
        "                v_h = v_h - self.vth_h * s_h\n",
        "                # Output dynamics\n",
        "                v_o = self.alpha_o * v_o + s_h @ self.W2\n",
        "                s_o_nat = k_wta(v_o, self.k_output)\n",
        "                v_o = v_o - self.vth_o * s_o_nat\n",
        "                # Teacher spike (supervision)\n",
        "                teach = 1.0 if rng.rand() < p_teacher else 0.0\n",
        "                y_vec = np.zeros(C, dtype=np.float32); y_vec[yb] = teach\n",
        "                # Potentiation uses union of natural + teacher\n",
        "                post_pos = np.maximum(s_o_nat, y_vec)\n",
        "                # Depression for non-target natural spikes\n",
        "                post_neg = s_o_nat.copy(); post_neg[yb] = 0.0\n",
        "                # Oja's rule for W1 (stability)\n",
        "                if s_h.any():\n",
        "                    J = np.where(s_h > 0)[0]\n",
        "                    x_col = x_t.astype(np.float32)[:, None]\n",
        "                    sj = s_h[J][None, :]\n",
        "                    self.W1[:, J] += eta1 * (x_col @ sj - (sj * sj) * self.W1[:, J])\n",
        "                # Supervised Hebbian for W2 with anti-Hebbian contrast\n",
        "                self.W2 += eta2 * np.outer(s_h, post_pos).astype(np.float32)\n",
        "                if eta2_neg > 0:\n",
        "                    self.W2 -= eta2_neg * np.outer(s_h, post_neg).astype(np.float32)\n",
        "                # Weight decay\n",
        "                if decay1 > 0: self.W1 *= (1.0 - decay1)\n",
        "                if decay2 > 0: self.W2 *= (1.0 - decay2)\n",
        "            # Column-wise normalization\n",
        "            def norm_cols(W):\n",
        "                n = np.linalg.norm(W, axis=0, keepdims=True) + 1e-6\n",
        "                W /= n\n",
        "            norm_cols(self.W1); norm_cols(self.W2)\n",
        "        # Clip\n",
        "        np.clip(self.W1, -1.0, 1.0, out=self.W1)\n",
        "        np.clip(self.W2, -1.0, 1.0, out=self.W2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the two-layer Hebbian SNN on a subset for speed\n",
        "input_dim = 28*28\n",
        "hidden_dim = 256\n",
        "num_classes = 10\n",
        "snn = TwoLayerHebbianSNN(input_dim, hidden_dim, num_classes, dt=0.005, tau_h=0.02, tau_o=0.02, vth_h=1.0, vth_o=1.0, seed=3, k_hidden=32, k_output=1)\n\n",
        "epochs = 20\n",
        "T = 20\n",
        "rng = np.random.RandomState(123)\n\n",
        "for ep in range(1, epochs+1):\n",
        "    for xb, yb in train_dp:\n",
        "        xb = scale01(xb)\n",
        "        y_int = yb.argmax(axis=1).astype(int)\n",
        "        spikes = poisson_encode(xb, T=T, rate_hz=30.0, dt=snn.dt, rng=rng)\n",
        "        snn.train_step(spikes, y_int, eta1=0.01, eta2=0.02, eta2_neg=0.01, teacher_rate_hz=80.0, decay1=5e-5, decay2=5e-5, rng=rng)\n",
        "    # Validate\n",
        "    correct_v, total_v = 0, 0\n",
        "    for xb, yb in valid_dp:\n",
        "        xb = scale01(xb)\n",
        "        y_int = yb.argmax(axis=1).astype(int)\n",
        "        spikes = poisson_encode(xb, T=T, rate_hz=30.0, dt=snn.dt, rng=rng)\n",
        "        preds = snn.predict(spikes)\n",
        "        correct_v += (preds == y_int).sum()\n",
        "        total_v += xb.shape[0]\n",
        "    acc = correct_v / max(1, total_v)\n",
        "    print(f'Epoch {ep:02d} | valid acc: {acc:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class templates by projecting W2 back to input space: W1 @ W2[:, c]\n",
        "plt.figure(figsize=(10, 3))\n",
        "classes_to_show = [0,1,2,3,4]\n",
        "for i, c in enumerate(classes_to_show):\n",
        "    plt.subplot(1, len(classes_to_show), i+1)\n",
        "    proto = (snn.W1 @ snn.W2[:, c]).reshape(28,28)\n",
        "    plt.imshow(proto, cmap='bwr')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'class {c}')\n",
        "plt.suptitle('Class templates (projected to input)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect spikes for a single example\n",
        "xb, yb = next(iter(valid_dp))\n",
        "xb = scale01(xb)\n",
        "label = int(yb[0].argmax())\n",
        "sp = poisson_encode(xb[:1], T=30, rate_hz=30.0, dt=snn.dt, rng=np.random.RandomState(0))  # (1, T, D)\n",
        "pred = int(snn.predict(sp)[0])\n",
        "print('True label:', label, 'Predicted:', pred)\n\n",
        "plt.figure(figsize=(6, 3))\n",
        "subset = 200\n",
        "sp_idx, t_idx = np.where(sp[0, :, :subset].T > 0)\n",
        "plt.scatter(t_idx, sp_idx, s=3, c='black')\n",
        "plt.xlabel('Time step')\n",
        "plt.ylabel('Input neuron (subset)')\n",
        "plt.title(f'Input spikes (subset) | true={label} pred={pred}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

