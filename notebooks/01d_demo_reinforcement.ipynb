{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01d: RL With Human Feedback (MNIST)\n\n",
    "This notebook demonstrates a small interactive loop that combines:\n\n",
    "- A conditional variational autoencoder (VAE) for digit generation (unsupervised pretraining).\n",
    "- A lightweight policy over latent codes updated with REINFORCE from human thumbs-up/down on generated images.\n",
    "- A simple classifier trained both with standard supervision (when available) and policy-gradient style updates from human correctness feedback.\n\n",
    "The goal is to provide an interactive demo where a human guides both generative and discriminative behavior via feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- Uses the repo's `mlp.data_providers.MNISTDataProvider` (expects `MLP_DATA_DIR` with `mnist-*.npz`).\n",
    "- Falls back to torchvision MNIST if data provider is unavailable.\n",
    "- Interactivity uses `ipywidgets`. If widgets don't render, install `ipywidgets` and enable JupyterLab extension if needed.\n",
    "- Training is kept intentionally small for demo speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and environment\n",
    "import os, math, time, random, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to import torch + widgets\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.distributions import Categorical\n",
    "except Exception as e:\n",
    "    print('PyTorch is required for this demo. Please install torch.')\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "except Exception as e:\n",
    "    print('ipywidgets not available. Install with `pip install ipywidgets`.')\n",
    "    raise\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 123\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# Ensure MLP_DATA_DIR is set if possible by searching common locations\n",
    "if 'MLP_DATA_DIR' not in os.environ:\n",
    "    candidates = [\n",
    "        Path.cwd() / 'data',\n",
    "        Path.cwd() / 'notebooks' / 'res',\n",
    "        Path.cwd().parent / 'data'\n",
    "    ]\n",
    "    for cand in candidates:\n",
    "        if (cand / 'mnist-train.npz').exists():\n",
    "            os.environ['MLP_DATA_DIR'] = str(cand.resolve())\n",
    "            break\n",
    "print('Using MLP_DATA_DIR =', os.environ.get('MLP_DATA_DIR', '<unset>'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading: prefer repo data provider, fallback to torchvision\n",
    "use_provider = False\n",
    "train_x = valid_x = train_y = valid_y = None\n",
    "\n",
    "try:\n",
    "    from mlp.data_providers import MNISTDataProvider\n",
    "    use_provider = True\n",
    "    print('Loading MNIST via MNISTDataProvider...')\n",
    "    train_dp = MNISTDataProvider('train', batch_size=256, max_num_batches=200, shuffle_order=True)\n",
    "    valid_dp = MNISTDataProvider('valid', batch_size=256, max_num_batches=50, shuffle_order=False)\n",
    "    # Aggregate a subset into memory for quick demo loops\n",
    "    def dp_to_arrays(dp):\n",
    "        xs, ys = [], []\n",
    "        for xb, yb in dp:\n",
    "            xs.append(xb.astype(np.float32))\n",
    "            ys.append(yb.astype(np.float32))\n",
    "        X = np.vstack(xs)\n",
    "        Y = np.vstack(ys)\n",
    "        return X, Y\n",
    "    train_x, train_y = dp_to_arrays(train_dp)\n",
    "    valid_x, valid_y = dp_to_arrays(valid_dp)\n",
    "    print('Train/Valid shapes:', train_x.shape, train_y.shape, '|', valid_x.shape, valid_y.shape)\n",
    "except Exception as e:\n",
    "    print('Falling back to torchvision MNIST due to:', e)\n",
    "    from torchvision import datasets, transforms\n",
    "    tfm = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_train = datasets.MNIST(root=str(Path.cwd()/'data'), train=True, download=True, transform=tfm)\n",
    "    mnist_test = datasets.MNIST(root=str(Path.cwd()/'data'), train=False, download=True, transform=tfm)\n",
    "    # Subset for speed\n",
    "    def subset(ds, n=15000):\n",
    "        idx = np.random.choice(len(ds), size=n, replace=False)\n",
    "        xs, ys = [], []\n",
    "        for i in idx:\n",
    "            x, y = ds[i]\n",
    "            xs.append(x.view(-1).numpy())\n",
    "            one = np.zeros(10, dtype=np.float32); one[y] = 1.0\n",
    "            ys.append(one)\n",
    "        return np.stack(xs).astype(np.float32), np.stack(ys).astype(np.float32)\n",
    "    train_x, train_y = subset(mnist_train, n=15000)\n",
    "    valid_x, valid_y = subset(mnist_test, n=3000)\n",
    "    print('Train/Valid shapes:', train_x.shape, train_y.shape, '|', valid_x.shape, valid_y.shape)\n",
    "\n",
    "IMG_SHAPE = (28, 28)\n",
    "INPUT_DIM = 28*28\n",
    "NUM_CLASSES = 10\n",
    "def to_img(arr):\n",
    "    return arr.reshape(IMG_SHAPE)\n",
    "\n",
    "# Torch tensors\n",
    "train_X_t = torch.from_numpy(train_x).to(DEVICE)\n",
    "train_Y_t = torch.from_numpy(train_y).to(DEVICE)\n",
    "valid_X_t = torch.from_numpy(valid_x).to(DEVICE)\n",
    "valid_Y_t = torch.from_numpy(valid_y).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional VAE (unsupervised pretraining)\n",
    "We train a small VAE conditioned on digit class. This serves as a base generator; RL will later select latent codes based on human reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional VAE\n",
    "class CondVAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, num_classes=10, z_dim=16, hidden=400):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.enc1 = nn.Linear(input_dim + num_classes, hidden)\n",
    "        self.enc_mu = nn.Linear(hidden, z_dim)\n",
    "        self.enc_logvar = nn.Linear(hidden, z_dim)\n",
    "        self.dec1 = nn.Linear(z_dim + num_classes, hidden)\n",
    "        self.dec_out = nn.Linear(hidden, input_dim)\n",
    "    def encode(self, x, y_onehot):\n",
    "        h = F.relu(self.enc1(torch.cat([x, y_onehot], dim=-1)))\n",
    "        mu = self.enc_mu(h)\n",
    "        logvar = self.enc_logvar(h)\n",
    "        return mu, logvar\n",
    "    def reparam(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def decode(self, z, y_onehot):\n",
    "        h = F.relu(self.dec1(torch.cat([z, y_onehot], dim=-1)))\n",
    "        logits = self.dec_out(h)\n",
    "        return logits\n",
    "    def forward(self, x, y_onehot):\n",
    "        mu, logvar = self.encode(x, y_onehot)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        logits = self.decode(z, y_onehot)\n",
    "        return logits, mu, logvar\n",
    "\n",
    "def vae_loss(x, logits, mu, logvar):\n",
    "    # Bernoulli likelihood over pixels\n",
    "    recon = F.binary_cross_entropy_with_logits(logits, x, reduction='sum')\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (recon + kld) / x.size(0)\n",
    "\n",
    "vae = CondVAE(input_dim=INPUT_DIM, num_classes=NUM_CLASSES, z_dim=16, hidden=400).to(DEVICE)\n",
    "opt_vae = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "def train_vae(epochs=3, batch=256):\n",
    "    vae.train()\n",
    "    N = train_X_t.size(0)\n",
    "    idxs = torch.randperm(N, device=DEVICE)\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for i in range(0, N, batch):\n",
    "            sel = idxs[i:i+batch]\n",
    "            xb = train_X_t[sel]\n",
    "            yb = train_Y_t[sel]\n",
    "            logits, mu, logvar = vae(xb, yb)\n",
    "            loss = vae_loss(xb, logits, mu, logvar)\n",
    "            opt_vae.zero_grad(); loss.backward(); opt_vae.step()\n",
    "            total += loss.item() * xb.size(0)\n",
    "        print(f'Epoch {ep+1}: loss={total/N:.4f}')\n",
    "\n",
    "train_vae(epochs=3, batch=256)  # keep small for demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Policy over Latents (RL)\n",
    "We freeze the decoder and learn a policy that selects among K latent codes per class to maximize human reward (thumbs up).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze VAE decoder for RL phase\n",
    "for p in vae.parameters():\n",
    "    p.requires_grad_(False)\n",
    "vae.eval()\n",
    "\n",
    "K = 8  # number of latent options per class\n",
    "class LatentPolicy(nn.Module):\n",
    "    def __init__(self, num_classes=10, z_dim=16, K=8):\n",
    "        super().__init__()\n",
    "        self.logits = nn.Parameter(torch.zeros(num_classes, K))\n",
    "        self.latents = nn.Parameter(0.1 * torch.randn(num_classes, K, z_dim))\n",
    "        self.baseline = torch.zeros(num_classes)  # moving avg baseline (not a Parameter)\n",
    "    def sample(self, c_idx):\n",
    "        probs = F.softmax(self.logits[c_idx], dim=-1)\n",
    "        dist = Categorical(probs=probs)\n",
    "        a = dist.sample()\n",
    "        z = self.latents[c_idx, a]\n",
    "        logp = dist.log_prob(a)\n",
    "        return z, a, logp\n",
    "\n",
    "gen_policy = LatentPolicy(NUM_CLASSES, z_dim=vae.z_dim, K=K).to(DEVICE)\n",
    "opt_policy = torch.optim.Adam(gen_policy.parameters(), lr=5e-3)\n",
    "\n",
    "def decode_logits(z, c):\n",
    "    # c is int\n",
    "    y = torch.zeros(1, NUM_CLASSES, device=DEVICE); y[0, c] = 1.0\n",
    "    with torch.no_grad():\n",
    "        logits = vae.decode(z.view(1,-1), y)\n",
    "    return logits.view(-1)\n",
    "\n",
    "def show_img(vec, title=None):\n",
    "    img = vec.detach().cpu().numpy().reshape(28,28)\n",
    "    plt.figure(figsize=(2,2)); plt.axis('off');\n",
    "    plt.imshow(img, cmap='gray');\n",
    "    if title: plt.title(title);\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widgets for generation\n",
    "c_dropdown = widgets.Dropdown(options=[(str(i), i) for i in range(10)], value=0, description='Digit:')\n",
    "btn_generate = widgets.Button(description='Generate', button_style='')\n",
    "btn_up = widgets.Button(description='üëç', button_style='success')\n",
    "btn_down = widgets.Button(description='üëé', button_style='danger')\n",
    "out_gen = widgets.Output()\n",
    "\n",
    "state = {'last': None}  # store (c, z, a, logp, img_probs)\n",
    "\n",
    "def on_generate(_):\n",
    "    out_gen.clear_output()\n",
    "    c = int(c_dropdown.value)\n",
    "    z, a, logp = gen_policy.sample(c)\n",
    "    logits = decode_logits(z, c)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    with out_gen:\n",
    "        show_img(probs, title=f'Generated digit {c} (option {int(a)})')\n",
    "    state['last'] = (c, z.detach(), a.detach(), logp, probs.detach())\n",
    "\n",
    "def rl_update(reward):\n",
    "    item = state.get('last', None)\n",
    "    if item is None:\n",
    "        return\n",
    "    c, z, a, logp, probs = item\n",
    "    c_idx = c\n",
    "    # Simple moving average baseline per class to reduce variance\n",
    "    b = gen_policy.baseline[c_idx].item()\n",
    "    gen_policy.baseline[c_idx] = 0.9 * gen_policy.baseline[c_idx] + 0.1 * reward\n",
    "    adv = reward - b\n",
    "    loss = -(adv) * logp\n",
    "    opt_policy.zero_grad(); loss.backward(); opt_policy.step()\n",
    "\n",
    "def on_up(_): rl_update(1.0)\n",
    "def on_down(_): rl_update(0.0)\n",
    "\n",
    "btn_generate.on_click(on_generate)\n",
    "btn_up.on_click(on_up)\n",
    "btn_down.on_click(on_down)\n",
    "display(widgets.HBox([c_dropdown, btn_generate, btn_up, btn_down]))\n",
    "display(out_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier + Human Feedback\n",
    "The classifier predicts a digit for shown images. Provide feedback:\n",
    "- If incorrect and true label is known, pick the correct label and we train with cross-entropy.\n",
    "- Otherwise, we can apply a simple policy-gradient update with reward 1/0 for correct/incorrect on the sampled class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, num_classes)\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(h)\n",
    "        return logits\n",
    "\n",
    "clf = Classifier(INPUT_DIM, 128, NUM_CLASSES).to(DEVICE)\n",
    "opt_clf = torch.optim.Adam(clf.parameters(), lr=1e-3)\n",
    "\n",
    "# Optionally warm-start the classifier a bit for nicer demos\n",
    "def warmstart_clf(steps=200, batch=256):\n",
    "    clf.train()\n",
    "    N = train_X_t.size(0)\n",
    "    for _ in range(steps):\n",
    "        idx = torch.randint(0, N, (batch,), device=DEVICE)\n",
    "        xb = train_X_t[idx]\n",
    "        yb = train_Y_t[idx]\n",
    "        logits = clf(xb)\n",
    "        target = yb.argmax(dim=-1)\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        opt_clf.zero_grad(); loss.backward(); opt_clf.step()\n",
    "    print('Warm-start complete')\n",
    "\n",
    "warmstart_clf(steps=200, batch=256)\n",
    "\n",
    "# Interactive classifier loop\n",
    "btn_next = widgets.Button(description='Next sample')\n",
    "btn_correct = widgets.Button(description='Correct', button_style='success')\n",
    "btn_incorrect = widgets.Button(description='Incorrect', button_style='danger')\n",
    "dd_true = widgets.Dropdown(options=[(str(i), i) for i in range(10)], description='True:')\n",
    "out_clf = widgets.Output()\n",
    "\n",
    "clf_state = {'xb': None, 'yb': None, 'pred': None, 'logp': None, 'act': None}\n",
    "\n",
    "def draw_sample():\n",
    "    i = np.random.randint(0, valid_X_t.size(0))\n",
    "    x = valid_X_t[i:i+1]\n",
    "    y = valid_Y_t[i:i+1]\n",
    "    return x, y\n",
    "\n",
    "def show_clf(x, pred, prob):\n",
    "    with out_clf:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(2,2)); plt.axis('off');\n",
    "        plt.imshow(x.view(28,28).cpu(), cmap='gray');\n",
    "        plt.title(f'Pred: {pred} (p={prob:.2f})')\n",
    "        plt.show()\n",
    "\n",
    "def on_next(_):\n",
    "    x, y = draw_sample()\n",
    "    logits = clf(x)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    dist = Categorical(probs=probs)\n",
    "    act = dist.sample()\n",
    "    logp = dist.log_prob(act)\n",
    "    pred = int(act.item())\n",
    "    prob = float(probs[0, pred].item())\n",
    "    clf_state.update({'xb': x, 'yb': y, 'pred': pred, 'logp': logp, 'act': act})\n",
    "    show_clf(x, pred, prob)\n",
    "\n",
    "def on_correct(_):\n",
    "    item = clf_state\n",
    "    if item['logp'] is None: return\n",
    "    # Reward 1 for sampled action\n",
    "    loss = -1.0 * item['logp']\n",
    "    opt_clf.zero_grad(); loss.backward(); opt_clf.step()\n",
    "\n",
    "def on_incorrect(_):\n",
    "    item = clf_state\n",
    "    if item['xb'] is None: return\n",
    "    # If user provides true label, use cross-entropy; else negative reward\n",
    "    if dd_true.value is not None:\n",
    "        logits = clf(item['xb'])\n",
    "        target = torch.tensor([dd_true.value], device=DEVICE)\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        opt_clf.zero_grad(); loss.backward(); opt_clf.step()\n",
    "    else:\n",
    "        loss = -0.0 * item['logp']  # zero reward (no update)\n",
    "\n",
    "btn_next.on_click(on_next)\n",
    "btn_correct.on_click(on_correct)\n",
    "btn_incorrect.on_click(on_incorrect)\n",
    "display(widgets.HBox([btn_next, btn_correct, btn_incorrect, dd_true]))\n",
    "display(out_clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- VAE pretraining is minimal; increase epochs for better quality.\n",
    "- Generator RL updates only the latent-selection policy (and latent codes), not the decoder.\n",
    "- Classifier RL uses sampled actions for proper policy-gradient updates; cross-entropy is used when a true label is provided.\n",
    "- For persistent improvements, consider saving/loading policy and classifier weights.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
